{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egBMMLGV_X_x"
   },
   "source": [
    "### Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1727179517871,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "WhhUTua487C-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iuXHvhLALwz"
   },
   "source": [
    "### Global attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1727179518371,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "wXZVhdp8-flF"
   },
   "outputs": [],
   "source": [
    "training_dataroot = 'lab1_basic_training.csv' # Training data file file named as 'lab1_basic_training.csv'\n",
    "testing_dataroot = 'lab1_basic_testing.csv'   # Testing data file named as 'lab1_basic_testing.csv'\n",
    "output_dataroot = 'lab1_basic.csv' # Output file will be named as 'lab1_basic.csv'\n",
    "\n",
    "training_datalist =  [] # Training datalist, saved as numpy array\n",
    "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
    "\n",
    "output_datalist =  [] # Your prediction, should be a list with 100 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyTqIRxQAtWj"
   },
   "source": [
    "### Load the Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1727179518371,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "KUzYjoq9AwRp"
   },
   "outputs": [],
   "source": [
    "# Read input csv to datalist\n",
    "with open(training_dataroot, newline='') as csvfile:\n",
    "  training_datalist = pd.read_csv(training_dataroot).to_numpy()\n",
    "\n",
    "with open(testing_dataroot, newline='') as csvfile:\n",
    "  testing_datalist = pd.read_csv(testing_dataroot).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFXG-axpAcom"
   },
   "source": [
    "### Implement the Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bqYH_MvBv4v"
   },
   "source": [
    "#### Step 1: Split Data\n",
    "Split data in *training_datalist* into training dataset and validation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1727179518371,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "6K2QUnt-A-1r"
   },
   "outputs": [],
   "source": [
    "def SplitData(data, split_ratio):\n",
    "    \"\"\"\n",
    "    Splits the given dataset into training and validation sets based on the specified split ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The dataset to be split. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n",
    "    - split_ratio (float): The ratio of the data to be used for training. For example, a value of 0.8 means 80% of the data will be used for training and the remaining 20% for validation.\n",
    "\n",
    "    Returns:\n",
    "    - training_data (numpy.ndarray): The portion of the dataset used for training.\n",
    "    - validation_data (numpy.ndarray): The portion of the dataset used for validation.\n",
    "\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    validation_data = []\n",
    "\n",
    "    # TODO\n",
    "    # Shuffle the data to ensure randomness\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # Calculate the split index based on the ratio\n",
    "    split_index = int(len(data) * split_ratio)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    training_data = data[:split_index]\n",
    "    validation_data = data[split_index:]\n",
    "\n",
    "    return training_data, validation_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-miSLyewCeME"
   },
   "source": [
    "#### Step 2: Preprocess Data\n",
    "Handle unreasonable data and missing data\n",
    "\n",
    "> Hint 1: Outliers and missing data can be addressed by either removing them or replacing them using statistical methods (e.g., the mean of all data).\n",
    "\n",
    "> Hint 2: Missing data are represented as `np.nan`, so functions like `np.isnan()` can be used to detect them.\n",
    "\n",
    "> Hint 3: Methods such as the Interquartile Range (IQR) can help detect outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1727179518371,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "jR4TYnwwCrci"
   },
   "outputs": [],
   "source": [
    "def PreprocessData(data):\n",
    "    \"\"\"\n",
    "    Preprocess the given dataset and return the result.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The dataset to preprocess. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n",
    "\n",
    "    Returns:\n",
    "    - preprocessedData (numpy.ndarray): Preprocessed data.\n",
    "    \"\"\"\n",
    "\n",
    "    preprocessedData = []\n",
    "\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    # only for advance\n",
    "    # # Handle gender column (second column)\n",
    "    # mapping = {'F': 1, 'M': 2}\n",
    "\n",
    "    # # Transform gender values\n",
    "    # gender_column = data[:, 1]  # Extract the gender column\n",
    "    # # Apply mapping: map 'F' to 1, 'M' to 2, and leave np.nan unchanged\n",
    "    # gender_column_mapped = np.where(gender_column == 'F', 1, np.where(gender_column == 'M', 2, gender_column))\n",
    "    # data[:, 1] = gender_column_mapped  # Update the dataset with mapped values\n",
    "\n",
    "    # -----------------------------------------------------------------------------------\n",
    "\n",
    "    # Convert data to a float type to handle np.nan and numerical operations\n",
    "    data = data.astype(float)\n",
    "\n",
    "    # Handle missing data: Replace np.nan with the mean of each column\n",
    "    col_means = np.nanmean(data, axis=0)  # Compute the mean of each column ignoring np.nan\n",
    "    inds = np.where(np.isnan(data))  # Find indices of np.nan values\n",
    "    data[inds] = np.take(col_means, inds[1])  # Replace np.nan with the column mean\n",
    "\n",
    "    # Handle outliers using Interquartile Range (IQR)\n",
    "    def remove_outliers(data):\n",
    "        # Calculate IQR for each column\n",
    "        Q1 = np.percentile(data, 25, axis=0)\n",
    "        Q3 = np.percentile(data, 75, axis=0)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Define bounds for outliers\n",
    "        lower_bound = Q1 - 2 * IQR\n",
    "        upper_bound = Q3 + 2 * IQR\n",
    "\n",
    "        # Find rows where all features are within bounds\n",
    "        within_bounds = np.all((data >= lower_bound) & (data <= upper_bound), axis=1)\n",
    "\n",
    "        # Return the data without outliers\n",
    "        return data[within_bounds]\n",
    "\n",
    "    # Remove outliers\n",
    "    preprocessedData = remove_outliers(data)\n",
    "\n",
    "    return preprocessedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csS9lL8DCzZO"
   },
   "source": [
    "### Step 3: Implement Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727179518371,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "n8ftprTRC0Na"
   },
   "outputs": [],
   "source": [
    "def Regression(dataset, degree):\n",
    "    \"\"\"\n",
    "    Performs regression on the given dataset and return the coefficients.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (numpy.ndarray): A 2D array where each row represents a data point.\n",
    "\n",
    "    Returns:\n",
    "    - w (numpy.ndarray): The coefficients of the regression model. For example, y = w[0] + w[1] * x + w[2] * x^2 + ...\n",
    "    \"\"\"\n",
    "\n",
    "    X = dataset[:, :-1] # All columns except the last one (features)\n",
    "    y = dataset[:, -1] # Last column (target)\n",
    "\n",
    "    # Standardize the training data (mean and std are for later use)\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X = (X - mean) / std\n",
    "\n",
    "    # Add polynomial features to X\n",
    "    X_poly = np.ones((X.shape[0], 1))  # Add intercept term (column of ones)\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly = np.hstack((X_poly, X ** d))  # Add x^d terms to feature matrix\n",
    "\n",
    "    # Initialize coefficients (weights) to zero\n",
    "    num_dimensions = X_poly.shape[1]  # Number of features (including intercept and polynomial terms)\n",
    "    print(f\"num_dimensions:{num_dimensions}\")\n",
    "    w = np.zeros(num_dimensions)\n",
    "    print(f\"X_poly.shape:{X_poly.shape}\")\n",
    "\n",
    "    # TODO: Set hyperparameters\n",
    "    if degree == 2 :\n",
    "      num_iteration = 30000\n",
    "      learning_rate = 0.002\n",
    "    if degree == 1 :\n",
    "      num_iteration = 500\n",
    "      learning_rate = 0.01\n",
    "\n",
    "    # Gradient Descent\n",
    "    m = len(y)  # Number of data points\n",
    "    for iteration in range(num_iteration):\n",
    "        # TODO: Prediction using current weights and compute error\n",
    "        y_pred = np.dot(X_poly, w)  # Prediction using current weights\n",
    "        error = y_pred - y  # Compute the error (residuals)\n",
    "\n",
    "        # TODO: Compute gradient\n",
    "        gradient = (1/m) * np.dot(X_poly.T, error)\n",
    "\n",
    "        # TODO: Update the weights\n",
    "        w = w - learning_rate * gradient\n",
    "\n",
    "        # Compute the cost (Mean Squared Error)\n",
    "        cost = (1/(2*m)) * np.sum(error ** 2)\n",
    "\n",
    "        # Optionally, print the cost every 100 iterations\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}, Cost: {cost}\")\n",
    "\n",
    "    return w, mean, std  # Return the weights, mean, and std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inqQ4lh8DFY6"
   },
   "source": [
    "### Step 4: Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727179518371,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "WwGE2qjgDLwt"
   },
   "outputs": [],
   "source": [
    "\n",
    "def MakePrediction(w, test_dataset, degree):\n",
    "    \"\"\"\n",
    "    Predicts the output for a given test dataset using a regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - w (numpy.ndarray): The coefficients of the model, where each element corresponds to\n",
    "                               a coefficient for the respective power of the independent variable.\n",
    "    - test_dataset (numpy.ndarray): A 1D array containing the input values (independent variable)\n",
    "                                          for which predictions are to be made.\n",
    "\n",
    "    Returns:\n",
    "    - list/numpy.ndarray: A list or 1d array of predicted values corresponding to each input value in the test dataset.\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    # Create a matrix with polynomial features for the test dataset\n",
    "    X_poly_test = np.ones((test_dataset.shape[0], 1))  # Start with a column of ones for the intercept term\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly_test = np.hstack((X_poly_test, test_dataset ** d))  # Add x^d terms\n",
    "\n",
    "    # Compute the predictions using the model's coefficients\n",
    "    prediction = np.dot(X_poly_test, w)  # dot product of the feature matrix and the weights\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-q4qKXbDDmG9"
   },
   "source": [
    "### Step 5: Train Model and Generate Result\n",
    "\n",
    "Use the above functions to train your model on training dataset, and predict the answer of testing dataset.\n",
    "\n",
    "Save your predicted values in `output_datalist`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1727179518372,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "coo82WvZDpMq",
    "outputId": "641e084d-25dd-456e-d3f6-26654dcde22f"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "split_ratio = 0.8  # For example, 80% for training and 20% for validation\n",
    "degree = 1         # Polynomial degree\n",
    "# (1) Split data\n",
    "training_data, validation_data = SplitData(training_datalist, split_ratio)\n",
    "\n",
    "# (2) Preprocess data\n",
    "training_data = PreprocessData(training_data)\n",
    "validation_data = PreprocessData(validation_data)\n",
    "\n",
    "# (3) Train regression model\n",
    "w, mean, std = Regression(training_data, degree)\n",
    "\n",
    "# (4) Predict validation dataset's answer, calculate MAPE comparing to the ground truth\n",
    "def MAPE(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Mean Absolute Percentage Error (MAPE) between the true and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (numpy.ndarray): Actual values (ground truth).\n",
    "    - y_pred (numpy.ndarray): Predicted values.\n",
    "\n",
    "    Returns:\n",
    "    - float: MAPE value as a percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "X_val = validation_data[:, :1]  # All columns except the last one (features)\n",
    "X_val = (X_val - mean) / std  # Standardize features\n",
    "y_val = validation_data[:, 1]   # Last column (target)\n",
    "\n",
    "# Make predictions on validation data\n",
    "y_val_pred = MakePrediction(w, X_val, degree)\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = MAPE(y_val, y_val_pred)\n",
    "print(f\"MAPE on validation data: {mape}%\")\n",
    "\n",
    "# (5) Make prediction of testing dataset and store the values in output_datalist\n",
    "\n",
    "# Standardize using training mean and std\n",
    "X_test = (testing_datalist - mean) / std\n",
    "\n",
    "# Make predictions on the standardized testing data\n",
    "output_datalist = MakePrediction(w, X_test, degree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW3NrFmGEEiG"
   },
   "source": [
    "### *Write the Output File*\n",
    "\n",
    "Write the prediction to output csv and upload the file to Kaggle\n",
    "> Format: 'Id', 'gripForce'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727179518372,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "Mo7rdhx0EFLn"
   },
   "outputs": [],
   "source": [
    "# Assume that output_datalist is a list (or 1d array) with length = 100\n",
    "\n",
    "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  writer.writerow(['Id', 'gripForce'])\n",
    "  for i in range(len(output_datalist)):\n",
    "    writer.writerow([i,output_datalist[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1O2l8d2E3he"
   },
   "source": [
    "# 2. Advanced Part (45%)\n",
    "In the second part, you need to implement regression differently from the basic part to improve your grip force predictions. You must use more than two features.\n",
    "\n",
    "You can choose either matrix inversion or gradient descent for this part\n",
    "\n",
    "We have provided `lab1_advanced_training.csv` for your training\n",
    "\n",
    "> Notice: Be cautious of the \"gender\" attribute, as it is represented by \"F\"/\"M\" rather than a numerical value.\n",
    "\n",
    "Please save the prediction result in a CSV file and submit it to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727179518372,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "K6hluh7UMB9C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727179518372,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "Us1kieIvcucL"
   },
   "outputs": [],
   "source": [
    "training_dataroot = 'lab1_advanced_training.csv' # Training data file file named as 'lab1_advanced_training.csv'\n",
    "testing_dataroot = 'lab1_advanced_testing.csv'   # Testing data file named as 'lab1_advanced_testing.csv'\n",
    "output_dataroot = 'lab1_advanced.csv' # Output file will be named as 'lab1_advanced.csv'\n",
    "\n",
    "training_datalist =  [] # Training datalist, saved as numpy array\n",
    "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
    "\n",
    "output_datalist =  [] # Your prediction, should be a list with 3000 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727179518372,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "zAmzFZM-dCkG"
   },
   "outputs": [],
   "source": [
    "# Read input csv to datalist\n",
    "with open(training_dataroot, newline='') as csvfile:\n",
    "  training_datalist = pd.read_csv(training_dataroot).to_numpy()\n",
    "\n",
    "with open(testing_dataroot, newline='') as csvfile:\n",
    "  testing_datalist = pd.read_csv(testing_dataroot).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8503,
     "status": "ok",
     "timestamp": 1727179526872,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "NSMzOXFAo2P0",
    "outputId": "1b794865-0f1e-4108-fd61-2fe62973556f"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "split_ratio = 0.8  # For example, 80% for training and 20% for validation\n",
    "degree = 2         # Polynomial degree\n",
    "# (1) Split data\n",
    "training_data, validation_data = SplitData(training_datalist, split_ratio)\n",
    "\n",
    "# (2) Preprocess data\n",
    "mapping = {'F': 1, 'M': 2}\n",
    "\n",
    "# Transform gender values\n",
    "gender_column = training_data[:, 1]  # Extract the gender column\n",
    "# Apply mapping: map 'F' to 1, 'M' to 2, and leave np.nan unchanged\n",
    "gender_column_mapped = np.where(gender_column == 'F', 1, np.where(gender_column == 'M', 2, gender_column))\n",
    "training_data[:, 1] = gender_column_mapped  # Update the dataset with mapped values\n",
    "\n",
    "# Transform gender values\n",
    "gender_column = validation_data[:, 1]  # Extract the gender column\n",
    "# Apply mapping: map 'F' to 1, 'M' to 2, and leave np.nan unchanged\n",
    "gender_column_mapped = np.where(gender_column == 'F', 1, np.where(gender_column == 'M', 2, gender_column))\n",
    "validation_data[:, 1] = gender_column_mapped  # Update the dataset with mapped values\n",
    "\n",
    "training_data = PreprocessData(training_data)\n",
    "validation_data = PreprocessData(validation_data)\n",
    "\n",
    "# (3) Train regression model\n",
    "w, mean, std = Regression(training_data, degree)\n",
    "\n",
    "# (4) Predict validation dataset's answer, calculate MAPE comparing to the ground truth\n",
    "def MAPE(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Mean Absolute Percentage Error (MAPE) between the true and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (numpy.ndarray): Actual values (ground truth).\n",
    "    - y_pred (numpy.ndarray): Predicted values.\n",
    "\n",
    "    Returns:\n",
    "    - float: MAPE value as a percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# (4) Predict validation dataset's answer, calculate MAPE comparing to the ground truth\n",
    "X_val = validation_data[:, :-1]  # All columns except the last one (features)\n",
    "X_val = (X_val - mean) / std  # Standardize features\n",
    "y_val = validation_data[:, -1]   # Last column (target)\n",
    "\n",
    "# Make predictions on validation data\n",
    "y_val_pred = MakePrediction(w, X_val, degree)\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = MAPE(y_val, y_val_pred)\n",
    "print(f\"MAPE on validation data: {mape}%\")\n",
    "\n",
    "# (5) Make prediction of testing dataset and store the values in output_datalist\n",
    "\n",
    "# Transform gender values\n",
    "gender_column = testing_datalist[:, 1]  # Extract the gender column\n",
    "# Apply mapping: map 'F' to 1, 'M' to 2, and leave np.nan unchanged\n",
    "gender_column_mapped = np.where(gender_column == 'F', 1, np.where(gender_column == 'M', 2, gender_column))\n",
    "testing_datalist[:, 1] = gender_column_mapped  # Update the dataset with mapped values\n",
    "\n",
    "# Standardize using training mean and std\n",
    "X_test = (testing_datalist - mean) / std\n",
    "\n",
    "# Make predictions on the standardized testing data\n",
    "output_datalist = MakePrediction(w, X_test, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVz38ASe-gGV"
   },
   "source": [
    "# Save the Code File\n",
    "Please save your code and submit it as an ipynb file! (**Lab1.ipynb**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1727179526872,
     "user": {
      "displayName": "張瑜庭",
      "userId": "02186097122319016792"
     },
     "user_tz": -480
    },
    "id": "d3Nzz4PijHP-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

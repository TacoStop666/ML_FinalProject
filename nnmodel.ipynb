{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQAqOS2xwiYG"
   },
   "source": [
    "Mount Google Drive (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "executionInfo": {
     "elapsed": 7645,
     "status": "ok",
     "timestamp": 1731235830070,
     "user": {
      "displayName": "賴詠欣",
      "userId": "11548507785211494753"
     },
     "user_tz": -480
    },
    "id": "fmTH9UkeqdYf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import csv\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# import re\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1731235836244,
     "user": {
      "displayName": "賴詠欣",
      "userId": "11548507785211494753"
     },
     "user_tz": -480
    },
    "id": "x0KHo8w9yqbY"
   },
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self, n_x, n_y, seed=1):\n",
    "        self.n_x = n_x\n",
    "        self.n_y = n_y\n",
    "        self.seed = seed\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        self.n_x -- size of the input layer\n",
    "        self.n_y -- size of the output layer\n",
    "        self.parameters -- python dictionary containing your parameters:\n",
    "                           W -- weight matrix of shape (n_x, n_y)\n",
    "                           b -- bias vector of shape (1, n_y)\n",
    "        \"\"\"\n",
    "        sd = np.sqrt(6.0 / (self.n_x + self.n_y))\n",
    "        np.random.seed(self.seed)\n",
    "        W = np.random.uniform(-sd, sd, (self.n_y, self.n_x)).T      # the transpose here is just for the code to be compatible with the old codes\n",
    "        b = np.zeros((1, self.n_y))\n",
    "\n",
    "        assert(W.shape == (self.n_x, self.n_y))\n",
    "        assert(b.shape == (1, self.n_y))\n",
    "\n",
    "        self.parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "        Arguments:\n",
    "        A -- activations from previous layer (or input data) with the shape (n, f^[l-1])\n",
    "        self.cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "\n",
    "        Returns:\n",
    "        Z -- the input of the activation function, also called pre-activation parameter with the shape (n, f^[l])\n",
    "        \"\"\"\n",
    "\n",
    "        # GRADED FUNCTION: linear_forward\n",
    "        ### START CODE HERE ###\n",
    "        w = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        Z = np.dot(A, w) + b\n",
    "        self.cache = (A, w, b)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(Z.shape == (A.shape[0], self.parameters[\"W\"].shape[1]))\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "        Arguments:\n",
    "        dZ -- Gradient of the loss with respect to the linear output (of current layer l), same shape as Z\n",
    "        self.cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "        self.dW -- Gradient of the loss with respect to W (current layer l), same shape as W\n",
    "        self.db -- Gradient of the loss with respect to b (current layer l), same shape as b\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- Gradient of the loss with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "\n",
    "        \"\"\"\n",
    "        A_prev, W, b = self.cache\n",
    "        m = A_prev.shape[0]\n",
    "\n",
    "        # GRADED FUNCTION: linear_backward\n",
    "        ### START CODE HERE ###\n",
    "        self.dW = np.dot(A_prev.T, dZ) / m\n",
    "        self.db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        dA_prev = np.dot(dZ, W.T)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert (dA_prev.shape == A_prev.shape)\n",
    "        assert (self.dW.shape == self.parameters[\"W\"].shape)\n",
    "        assert (self.db.shape == self.parameters[\"b\"].shape)\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent\n",
    "\n",
    "        Arguments:\n",
    "        learning rate -- step size\n",
    "        \"\"\"\n",
    "\n",
    "        # GRADED FUNCTION: linear_update_parameters\n",
    "        ### START CODE HERE ###\n",
    "        self.parameters[\"W\"] = self.parameters[\"W\"] - learning_rate * self.dW\n",
    "        self.parameters[\"b\"] = self.parameters[\"b\"] - learning_rate * self.db\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1731235843574,
     "user": {
      "displayName": "賴詠欣",
      "userId": "11548507785211494753"
     },
     "user_tz": -480
    },
    "id": "Nnuv8MmebMgg"
   },
   "outputs": [],
   "source": [
    "class Activation():\n",
    "    def __init__(self, activation_function, loss_function):\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            \"\"\"\n",
    "            Implements the sigmoid activation in numpy\n",
    "\n",
    "            Arguments:\n",
    "            Z -- numpy array of any shape\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "\n",
    "            Returns:\n",
    "            A -- output of sigmoid(z), same shape as Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: sigmoid_forward\n",
    "            ### START CODE HERE ###\n",
    "            A = 1 / (1 + np.exp(-Z))\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"relu\":\n",
    "            \"\"\"\n",
    "            Implement the RELU function in numpy\n",
    "            Arguments:\n",
    "            Z -- numpy array of any shape\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "            Returns:\n",
    "            A -- output of relu(z), same shape as Z\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: relu_forward\n",
    "            ### START CODE HERE ###\n",
    "            A = np.maximum(0, Z)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert(A.shape == Z.shape)\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            \"\"\"\n",
    "            Implements the softmax activation in numpy\n",
    "\n",
    "            Arguments:\n",
    "            Z -- np.array with shape (n, C)\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "\n",
    "            Returns:\n",
    "            A -- output of softmax(z), same shape as Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: softmax_forward\n",
    "            ### START CODE HERE ###\n",
    "            eZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "            A = eZ / np.sum(eZ, axis=1, keepdims=True)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"linear\":\n",
    "            \"\"\"\n",
    "            Linear activation (returns Z directly).\n",
    "            \"\"\"\n",
    "            self.cache = Z.copy()\n",
    "            return Z\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")\n",
    "\n",
    "\n",
    "    def backward(self, dA=None, Y=None):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a single SIGMOID unit.\n",
    "            Arguments:\n",
    "            dA -- post-activation gradient, of any shape\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the loss with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: sigmoid_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            sigmoid = 1 / (1 + np.exp(-Z))\n",
    "            dZ = dA * sigmoid * (1 - sigmoid)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"relu\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a single RELU unit.\n",
    "            Arguments:\n",
    "            dA -- post-activation gradient, of any shape\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the loss with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: relu_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            dZ = np.array(dA, copy=True)\n",
    "            dZ[Z <= 0] = 0\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a [SOFTMAX->CCE LOSS] unit.\n",
    "            Arguments:\n",
    "            Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
    "                                      in a Rock-Paper-Scissors, shape: (n, C)\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the cost with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: softmax_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            softmax = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "            softmax = softmax / np.sum(softmax, axis=1, keepdims=True)\n",
    "            dZ = softmax - Y\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == self.cache.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"linear\":\n",
    "            \"\"\"\n",
    "            Backward propagation for linear activation.\n",
    "            \"\"\"\n",
    "            return dA\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1731235849661,
     "user": {
      "displayName": "賴詠欣",
      "userId": "11548507785211494753"
     },
     "user_tz": -480
    },
    "id": "0JGMzfIDCSVz"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, units, activation_functions, loss_function):\n",
    "        self.units = units\n",
    "        self.activation_functions = activation_functions\n",
    "        self.loss_function = loss_function\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize layers of the neural network\n",
    "\n",
    "        Arguments:\n",
    "            self.units -- array defining network structure (e.g., [4,4,1]):\n",
    "                - Input layer: 4 nodes\n",
    "                - Hidden layer: 4 nodes\n",
    "                - Output layer: 1 node\n",
    "            self.activation_functions -- activation function for each layer (e.g., [\"relu\",\"sigmoid\"]):\n",
    "                - First layer uses ReLU\n",
    "                - Second layer uses Sigmoid\n",
    "            self.loss_function -- loss function type: \"cross_entropy\" or \"mse\"\n",
    "        \"\"\"\n",
    "        self.linear = []        # Store all Dense layers (weights & biases)\n",
    "        self.activation = []    # Store all activation function layers\n",
    "\n",
    "        for i in range(len(self.units)-1):\n",
    "            dense = Dense(self.units[i], self.units[i+1], i)\n",
    "            self.linear.append(dense)\n",
    "\n",
    "        for i in range(len(self.activation_functions)):\n",
    "            self.activation.append(Activation(self.activation_functions[i], self.loss_function))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network\n",
    "\n",
    "        Arguments:\n",
    "        X -- input data: shape (n, f)\n",
    "        Returns:\n",
    "        A -- model output:\n",
    "            - For binary classification: probability (0-1)\n",
    "            - For multi-class: probability distribution across classes\n",
    "            - For regression: predicted values\n",
    "        \"\"\"\n",
    "        A = X\n",
    "\n",
    "        # GRADED FUNCTION: model_forward\n",
    "        ### START CODE HERE ###\n",
    "        for i in range(len(self.linear)):\n",
    "          Z = self.linear[i].forward(A) # forward\n",
    "          A = self.activation[i].forward(Z) # activation\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, AL=None, Y=None):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients\n",
    "\n",
    "        Arguments:\n",
    "            AL -- model output from forward propagation:\n",
    "                - For binary: probability (n,1)\n",
    "                - For multi-class: probabilities (n,C)\n",
    "            Y -- true labels:\n",
    "                - For binary: 0/1 labels (n,1)\n",
    "                - For multi-class: one-hot vectors (n,C)\n",
    "                - For regression: true values (n,1)\n",
    "\n",
    "        Returns:\n",
    "            dA_prev -- gradients for previous layer's activation\n",
    "        \"\"\"\n",
    "\n",
    "        L = len(self.linear)\n",
    "        C = Y.shape[1]\n",
    "\n",
    "        # assertions\n",
    "        warning = 'Warning: only the following 3 combinations are allowed! \\n \\\n",
    "                    1. binary classification: sigmoid + cross_entropy \\n \\\n",
    "                    2. multi-class classification: softmax + cross_entropy \\n \\\n",
    "                    3. regression: linear + mse'\n",
    "        assert self.loss_function in [\"cross_entropy\", \"mse\"], \"you're using undefined loss function!\"\n",
    "        if self.loss_function == \"cross_entropy\":\n",
    "            if Y.shape[1] == 1:  # binary classification\n",
    "                assert self.activation_functions[-1] == 'sigmoid', warning\n",
    "            else:  # multi-class classification\n",
    "                assert self.activation_functions[-1] == 'softmax', warning\n",
    "                assert self.units[-1] == Y.shape[1], f\"you should set last dim to {Y.shape[1]}(the number of classes) in multi-class classification!\"\n",
    "        elif self.loss_function == \"mse\":\n",
    "            assert self.activation_functions[-1] == 'linear', warning\n",
    "            assert self.units[-1] == Y.shape[1], \"output dimension mismatch for regression!\"\n",
    "\n",
    "        # GRADED FUNCTION: model_backward\n",
    "        ### START CODE HERE ###\n",
    "        dAL = 0.0\n",
    "        # Initializing the backpropagation\n",
    "        if self.loss_function == \"cross_entropy\":\n",
    "          dAL = (-1 * (Y / (AL + 0.00001))) + ((1 - Y) / (1 - AL + 0.00001))\n",
    "        elif self.loss_function == \"mse\":\n",
    "          dAL = AL - Y\n",
    "\n",
    "        if self.activation_functions[-1] == \"linear\":\n",
    "          # Lth layer (LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "          dZ = dAL\n",
    "          dA_prev = self.linear[-1].backward(dZ)\n",
    "\n",
    "        elif self.activation_functions[-1] == \"sigmoid\":\n",
    "          # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "          dZ = self.activation[-1].backward(dAL)\n",
    "          dA_prev = self.linear[-1].backward(dZ)\n",
    "\n",
    "        elif self.activation_functions[-1] == \"softmax\":\n",
    "          dZ = self.activation[-1].backward(Y=Y)\n",
    "          dA_prev = self.linear[-1].backward(dZ)\n",
    "\n",
    "        # Loop from l=L-2 to l=0\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n",
    "        for l in reversed(range(L - 1)):\n",
    "          dZ = self.activation[l].backward(dA_prev)\n",
    "          dA_prev = self.linear[l].backward(dZ)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        learning_rate -- step size\n",
    "        \"\"\"\n",
    "\n",
    "        L = len(self.linear)\n",
    "\n",
    "        # GRADED FUNCTION: model_update_parameters\n",
    "        ### START CODE HERE ###\n",
    "        for l in self.linear:\n",
    "          l.update(learning_rate)\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1731235865258,
     "user": {
      "displayName": "賴詠欣",
      "userId": "11548507785211494753"
     },
     "user_tz": -480
    },
    "id": "6RHLNGsepygt"
   },
   "outputs": [],
   "source": [
    "def compute_MSE_loss(AL, Y):\n",
    "    # 檢查 AL 和 Y 是否有 NaN 或無效值\n",
    "    if np.any(np.isnan(AL)) or np.any(np.isnan(Y)):\n",
    "        raise ValueError(\"AL or Y contains NaN values!\")\n",
    "    if np.any(np.isinf(AL)) or np.any(np.isinf(Y)):\n",
    "        raise ValueError(\"AL or Y contains Inf values!\")\n",
    "    \n",
    "    # 防止 m 為 0\n",
    "    m = Y.shape[0]\n",
    "    if m == 0:\n",
    "        raise ValueError(\"Number of samples (m) is zero, cannot compute loss.\")\n",
    "    \n",
    "    # 計算 MSE loss\n",
    "    loss = (1 / m) * np.sum(np.square(AL - Y))\n",
    "    return loss\n",
    "\n",
    "# compute_MSE_loss (MSE)\n",
    "def compute_MSE_loss(AL, Y):\n",
    "    m = Y.shape[0]\n",
    "    loss = (1/m) * np.sum(np.square(AL - Y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1731235865258,
     "user": {
      "displayName": "賴詠欣",
      "userId": "11548507785211494753"
     },
     "user_tz": -480
    },
    "id": "woCqucFUYXe6"
   },
   "outputs": [],
   "source": [
    "# def predict(x, y_true, model):\n",
    "#     \"\"\"\n",
    "#     This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "#     Arguments:\n",
    "#     x -- data set of examples you would like to label\n",
    "#     model -- trained model\n",
    "\n",
    "#     Returns:\n",
    "#     y_pred -- predictions for the given dataset X\n",
    "#     \"\"\"\n",
    "\n",
    "#     n = x.shape[0]\n",
    "\n",
    "#     # Forward propagation\n",
    "#     y_pred = model.forward(x)\n",
    "\n",
    "#     # this transform the output and label of binary classification when using sigmoid + cross entropy for evaluation\n",
    "#     # eg. y_pred: [[0.8], [0.2], [0.1]] -> [[0.2, 0.8], [0.8, 0.2], [0.9, 0.1]]\n",
    "#     # eg. y_true: [[1], [0], [0]] -> [[0, 1], [1, 0], [1, 0]]\n",
    "#     if y_pred.shape[-1] == 1:\n",
    "#         y_pred = np.array([[1 - y[0], y[0]] for y in y_pred])\n",
    "#         if y_true is not None:\n",
    "#             y_true = np.array([[1,0] if y == 0 else [0,1] for y in y_true.reshape(-1)])\n",
    "\n",
    "#     # make y_pred/y_true become one-hot prediction result\n",
    "#     # eg. y_true: [[1, 0, 0], [0, 0, 1], [0, 1, 0]] -> [0, 2, 1]\n",
    "#     # eg. y_pred: [[0.2, 0.41, 0.39], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]] -> [1, 1, 2]\n",
    "#     if y_true is not None:\n",
    "#         y_true = np.argmax(y_true, axis=1)\n",
    "#     y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "#     if y_true is not None:\n",
    "#         # compute accuracy\n",
    "#         correct = 0\n",
    "#         for yt, yp in zip(y_true, y_pred):\n",
    "#             if yt == yp:\n",
    "#                 correct += 1\n",
    "#         print(f\"Accuracy: {correct/n * 100:.2f}%\")\n",
    "\n",
    "#         # f1_scores = f1_score(y_true, y_pred, average=None)\n",
    "#         # print(f'f1 score for each class: {f1_scores}')\n",
    "#         # print(f'f1_macro score: {np.mean(np.array(f1_scores)):.2f}')\n",
    "#         MAPE = calculate_MAPE(y_pred, y_true)\n",
    "#         print(f\"MAPE: {MAPE}\")\n",
    "\n",
    "#     return y_pred\n",
    "\n",
    "def save_prediction_data(predicted_y):\n",
    "    # Create DataFrame with ID, x, and y columns\n",
    "    df = pd.DataFrame({\n",
    "        'ID': range(len(predicted_y)),  # Add ID column starting from 0\n",
    "        'y': predicted_y\n",
    "    })\n",
    "\n",
    "    # Ensure ID is the first column\n",
    "    df = df[['ID', 'y']]\n",
    "\n",
    "    # Save to CSV file\n",
    "    df.to_csv('Lab4_basic_regression.csv', index=False)\n",
    "    print(\"Prediction data saved as 'Lab4_basic_regression.csv'\")\n",
    "\n",
    "def animate_training(history, X_train, Y_train):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 11)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    line, = ax.plot([], [], 'b-', lw=1, label='Predicted')\n",
    "\n",
    "    ground_truth_x = X_train.flatten()\n",
    "    ground_truth_y = Y_train.flatten()\n",
    "    ax.plot(ground_truth_x, ground_truth_y, 'r-', lw=1, label='Ground Truth')\n",
    "\n",
    "    # show current epoch on the animation / 100 epoch\n",
    "    epoch_text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        epoch_text.set_text('')\n",
    "        return line, epoch_text\n",
    "\n",
    "    def update(frame):\n",
    "        epoch = (frame + 1) * 100\n",
    "        _, predicted_y = history[frame]\n",
    "        predicted_x = X_train.flatten()\n",
    "        line.set_data(predicted_x, predicted_y.flatten())\n",
    "\n",
    "        epoch_text.set_text(f'Epoch: {epoch}')\n",
    "\n",
    "        return line, epoch_text\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(history), init_func=init, blit=True, interval=50)\n",
    "\n",
    "    # save as gif\n",
    "    ani.save('Lab4_basic_regression.gif', writer='pillow')\n",
    "    plt.close(fig)\n",
    "    print(f\"Animation saved as 'Lab4_basic_regression.gif'\")\n",
    "\n",
    "\n",
    "def save_final_result(model, X_train, Y_train):\n",
    "    AL = model.forward(X_train)\n",
    "\n",
    "    predicted_x = X_train.flatten()\n",
    "    predicted_y = AL.flatten()\n",
    "\n",
    "    plt.plot(predicted_x, predicted_y, 'b-', label=\"Predicted\", lw=1)\n",
    "\n",
    "    ground_truth_x = X_train.flatten()\n",
    "    ground_truth_y = Y_train.flatten()\n",
    "\n",
    "    save_prediction_data(predicted_y)\n",
    "\n",
    "    plt.plot(ground_truth_x, ground_truth_y, 'r-', label='Ground Truth', lw=1)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.xlim(0, 11)\n",
    "    plt.savefig(\"Lab4_basic_regression.jpg\")\n",
    "    plt.show()\n",
    "    print(\"Prediction saved as 'Lab4_basic_regression.jpg'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1731235868764,
     "user": {
      "displayName": "賴詠欣",
      "userId": "11548507785211494753"
     },
     "user_tz": -480
    },
    "id": "fjOBHI0bGVE7"
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (n, f^{0})\n",
    "    Y -- true \"label\" vector, of shape (n, C)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    # permutation = list(np.random.permutation(m))\n",
    "    # shuffled_X = X[permutation, :]\n",
    "    # shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    permutation = np.random.permutation(m)\n",
    "    shuffled_X = X[permutation]\n",
    "    shuffled_Y = Y[permutation]\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        # mini_batch_X = shuffled_X[k * mini_batch_size : m - 1]\n",
    "        # mini_batch_Y = shuffled_Y[k * mini_batch_size : m - 1]\n",
    "        # mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        # mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "def train_model(model, X_train, Y_train, learning_rate, num_iterations, batch_size=None, print_loss=True, print_freq=1000, decrease_freq=100, decrease_proportion=0.99):\n",
    "    \"\"\"\n",
    "    Trains the model using mini-batch gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    model -- the model to be trained\n",
    "    X_train -- training set, of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels, of shape (1, m_train)\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    batch_size -- size of a mini batch\n",
    "    print_loss -- if True, print the loss every print_freq iterations\n",
    "    print_freq -- print frequency\n",
    "    decrease_freq -- learning rate decrease frequency\n",
    "    decrease_proportion -- learning rate decrease proportion\n",
    "\n",
    "    Returns:\n",
    "    model -- the trained model\n",
    "    losses -- list of losses computed during the optimization\n",
    "    history -- list of (X_train, Y_pred) tuples for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    history = []\n",
    "    losses = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        ### START CODE HERE ###\n",
    "        # Define mini batches\n",
    "        mini_batches = [(X_train, Y_train)]\n",
    "        if batch_size:\n",
    "            mini_batches = random_mini_batches(X_train, Y_train, batch_size)\n",
    "        else:\n",
    "            # if batch_size is None, batch is not used, mini_batch = whole dataset\n",
    "            mini_batches = [(X_train, Y_train)]\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for batch in mini_batches:\n",
    "            X_batch, Y_batch = batch\n",
    "\n",
    "            # Forward pass\n",
    "            AL = model.forward(X_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            # if model.loss_function == 'cross_entropy':\n",
    "            #     if model.activation_functions[-1] == \"sigmoid\": # Binary classification\n",
    "            #         loss = compute_BCE_loss(AL, Y_batch)\n",
    "            #     elif model.activation_functions[-1] == \"softmax\": # Multi-class classification\n",
    "            #         loss = compute_CCE_loss(AL, Y_batch)\n",
    "            # elif model.loss_function == 'mse': # Regression\n",
    "            #     loss = compute_MSE_loss(AL, Y_batch)\n",
    "            loss = compute_MSE_loss(AL, Y_batch)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            model.backward(AL, Y_batch)\n",
    "\n",
    "            # Update parameters\n",
    "            model.update(learning_rate)\n",
    "\n",
    "        epoch_loss /= len(mini_batches)\n",
    "        losses.append(epoch_loss)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Print loss\n",
    "        if print_loss and i % print_freq == 0:\n",
    "            print(f\"Loss after iteration {i}: {epoch_loss}\")\n",
    "\n",
    "        # Store history\n",
    "        if i % 100 == 0:\n",
    "            history.append((X_train, model.forward(X_train)))\n",
    "\n",
    "        # Decrease learning rate\n",
    "        if i % decrease_freq == 0 and i > 0:\n",
    "            learning_rate *= decrease_proportion\n",
    "\n",
    "    return model, losses, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessor(text):\n",
    "#     # remove HTML tags\n",
    "#     text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "#     # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "#     r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "#     emoticons = re.findall(r, text)\n",
    "#     text = re.sub(r, '', text)\n",
    "\n",
    "#     # convert to lowercase and append all emoticons behind (with space in between)\n",
    "#     # replace('-','') removes nose of emoticons\n",
    "#     text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "#     return text\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# stop = stopwords.words('english')\n",
    "\n",
    "# def tokenizer_stem_nostop(text):\n",
    "#     porter = PorterStemmer()\n",
    "#     return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "#             if w not in stop and re.match('[a-zA-Z]+', w)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessed(x, y):\n",
    "    y = y[:, np.newaxis]\n",
    "    combined = np.hstack((x, y))\n",
    "    nan_removed = combined[~np.isnan(combined).any(axis=1)]\n",
    "    x_train = nan_removed[:, :-1]\n",
    "    y_train = nan_removed[:, -1:]\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    y_train = scaler.fit_transform(y_train)\n",
    "    x_train[:, 0] = np.where(x_train[:, 0] > 0, np.log10(x_train[:, 0].clip(min=1e-10)), x_train[:, 0])\n",
    "    x_train[:, 1] = np.where(x_train[:, 1] > 0, np.log10(x_train[:, 1].clip(min=1e-10)), x_train[:, 1])\n",
    "    x_train[:, 2] = np.where(x_train[:, 2] > 0, np.log10(x_train[:, 2].clip(min=1e-10)), x_train[:, 2])\n",
    "    y_train[:, 0] = np.where(y_train[:, 0] > 0, np.log10(y_train[:, 0].clip(min=1e-10)), y_train[:, 0])\n",
    "    # x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)\n",
    "    # y_train = (y_train - np.mean(y_train)) / np.std(y_train)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X_train, Y_train, split_ratio=0.2):\n",
    "\n",
    "    x_split_size = int(len(X_train) * split_ratio)\n",
    "    y_split_size = int(len(Y_train) * split_ratio)\n",
    "    x_train = X_train[x_split_size:]\n",
    "    y_train = Y_train[y_split_size:]\n",
    "    x_val = X_train[:x_split_size]\n",
    "    y_val = Y_train[:y_split_size]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "data_root = \"out.csv\"\n",
    "with open(data_root, newline='') as csvfile:\n",
    "    datalist = pd.read_csv(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = CountVectorizer(ngram_range=(1, 1),\n",
    "#                         preprocessor=preprocessor,\n",
    "#                         tokenizer=tokenizer_stem_nostop)\n",
    "# datal = datalist.to_numpy()\n",
    "# count.fit(datal[:,7])\n",
    "# BoW = count.vocabulary_\n",
    "# print('[vocabulary]\\n{}'.format(BoW))\n",
    "# doc_bag = count.transform(datal[:,7])\n",
    "# doc_bag = doc_bag.toarray()\n",
    "# print(doc_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "executionInfo": {
     "elapsed": 479,
     "status": "error",
     "timestamp": 1731045408147,
     "user": {
      "displayName": "賴詠欣",
      "userId": "11548507785211494753"
     },
     "user_tz": -480
    },
    "id": "alsJ4F6eHZ2a",
    "outputId": "fba35e56-d600-47a9-981d-15fa7aa36964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (2670, 3), y_train shape: (2670, 1)\n",
      "x_val shape: (667, 3), y_val shape: (667, 1)\n",
      "Loss after iteration 0: 0.07731880125409621\n",
      "Loss after iteration 1000: 0.04317019979105179\n",
      "Loss after iteration 2000: 0.037076283561504754\n",
      "Loss after iteration 3000: 0.033610420908283034\n",
      "Loss after iteration 4000: 0.030786399376077928\n",
      "Loss after iteration 5000: 0.029466616134195826\n",
      "Loss after iteration 6000: 0.027927002379815154\n",
      "Loss after iteration 7000: 0.02707632950089545\n",
      "Loss after iteration 8000: 0.0264759394893279\n",
      "Loss after iteration 9000: 0.025120211281241235\n",
      "Loss after iteration 10000: 0.025276023554482115\n",
      "Loss after iteration 11000: 0.024454584645825755\n",
      "Loss after iteration 12000: 0.024079671138055377\n",
      "Loss after iteration 13000: 0.023603161890341215\n",
      "Loss after iteration 14000: 0.02338877229991053\n",
      "Loss after iteration 15000: 0.022977514565271435\n",
      "Loss after iteration 16000: 0.02275206659842918\n",
      "Loss after iteration 17000: 0.022452425987628286\n",
      "Loss after iteration 18000: 0.022390108675030804\n",
      "Loss after iteration 19000: 0.022096557219975037\n",
      "Loss after iteration 20000: 0.022067706532492327\n",
      "Loss after iteration 21000: 0.021854435540004966\n",
      "Loss after iteration 22000: 0.02164067056545944\n",
      "Loss after iteration 23000: 0.021382110700358548\n",
      "Loss after iteration 24000: 0.021434194159104886\n",
      "Loss after iteration 25000: 0.021310018710963295\n",
      "Loss after iteration 26000: 0.02117447319416636\n",
      "Loss after iteration 27000: 0.021090638828281626\n",
      "Loss after iteration 28000: 0.020906564880433777\n",
      "Loss after iteration 29000: 0.020963401136498954\n",
      "Loss after iteration 30000: 0.020858807899711097\n",
      "Loss after iteration 31000: 0.02073915508338193\n",
      "Loss after iteration 32000: 0.020712545280913403\n",
      "Loss after iteration 33000: 0.02075945742120898\n",
      "Loss after iteration 34000: 0.020658281809844612\n",
      "Loss after iteration 35000: 0.02062989411706183\n",
      "Loss after iteration 36000: 0.02056577477702118\n",
      "Loss after iteration 37000: 0.020530585662478826\n",
      "Loss after iteration 38000: 0.02049427002503779\n",
      "Loss after iteration 39000: 0.020535141837201846\n",
      "Loss after iteration 40000: 0.020463111209877722\n",
      "Loss after iteration 41000: 0.020447716325708527\n",
      "Loss after iteration 42000: 0.020452429114750464\n",
      "Loss after iteration 43000: 0.020405028564472735\n",
      "Loss after iteration 44000: 0.02039653665882226\n",
      "Loss after iteration 45000: 0.020373852126819532\n",
      "Loss after iteration 46000: 0.020359076555922603\n",
      "Loss after iteration 47000: 0.020344922879021576\n",
      "Loss after iteration 48000: 0.020325392474190247\n",
      "Loss after iteration 49000: 0.02033675469956522\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filtered_data = datalist[(datalist[\"followers\"] > 10000) & (datalist[\"followers\"] < 1000000)]\n",
    "# data_isVideo = filtered_data[filtered_data[\"is Photo\"] == 0]\n",
    "data_isVideo = datalist[(datalist[\"followers\"] > 100) & (datalist[\"followers\"] < 1000000) & (datalist[\"is Video\"] == 1)]\n",
    "# xy = filtered_data[[ \"followers\",\"is Photo\", \"hashtag_count\", \"days_diff\", \"Like Count\"]].to_numpy()\n",
    "xy_isVideo = data_isVideo[[ \"followers\", \"hashtag_count\", \"days_diff\", \"Like Count\"]].to_numpy()\n",
    "\n",
    "x_isVideo = xy_isVideo[:, :-1]  \n",
    "y_isVideo = xy_isVideo[:, -1]   \n",
    "x_processed_isVideo, y_processed_isVideo = preProcessed(x_isVideo, y_isVideo)\n",
    "x_train_isVideo, y_train_isVideo, x_val_isVideo, y_val_isVideo = split_data(x_processed_isVideo, y_processed_isVideo, split_ratio=0.2)\n",
    "print(f\"x_train shape: {x_train_isVideo.shape}, y_train shape: {y_train_isVideo.shape}\")\n",
    "print(f\"x_val shape: {x_val_isVideo.shape}, y_val shape: {y_val_isVideo.shape}\")\n",
    "\n",
    "loss_function = \"mse\";\n",
    "layers_dims = [x_train_isVideo.shape[-1], 32, 16, 1] # linear for converge, sigmoid for diverge\n",
    "activation_fn = [\"relu\", \"relu\", \"linear\"]\n",
    "learning_rate = 0.05\n",
    "num_iterations = 50000\n",
    "print_loss = True\n",
    "print_freq = 1000\n",
    "decrease_freq = 1000\n",
    "decrease_proportion = 0.9\n",
    "batch_size = 16 \n",
    "\n",
    "model_isVideo = Model(layers_dims, activation_fn, loss_function)\n",
    "model_isVideo, losses, history = train_model(model_isVideo, x_train_isVideo, y_train_isVideo, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1572, 3), y_train shape: (1572, 1)\n",
      "x_val shape: (392, 3), y_val shape: (392, 1)\n",
      "Loss after iteration 0: 0.0022333453759689264\n",
      "Loss after iteration 1000: 2.760586949373095e-06\n",
      "Loss after iteration 2000: 1.6100113947689027e-06\n",
      "Loss after iteration 3000: 1.206897926083247e-06\n",
      "Loss after iteration 4000: 1.0164726434786984e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_isPhoto = filtered_data[filtered_data[\"is Photo\"] == 1]\n",
    "data_isPhoto = datalist[(datalist[\"followers\"] > 10000) & (datalist[\"followers\"] < 1000000) & (datalist[\"is Photo\"] == 1)]\n",
    "xy_isPhoto = data_isPhoto[[ \"followers\", \"hashtag_count\", \"days_diff\", \"Like Count\"]].to_numpy()\n",
    "\n",
    "x_isPhoto = xy_isPhoto[:, :-1]  \n",
    "y_isPhoto = xy_isPhoto[:, -1]   \n",
    "x_processed_isPhoto, y_processed_isPhoto = preProcessed(x_isPhoto, y_isPhoto)\n",
    "x_train_isPhoto, y_train_isPhoto, x_val_isPhoto, y_val_isPhoto = split_data(x_processed_isPhoto, y_processed_isPhoto, split_ratio=0.2)\n",
    "print(f\"x_train shape: {x_train_isPhoto.shape}, y_train shape: {y_train_isPhoto.shape}\")\n",
    "print(f\"x_val shape: {x_val_isPhoto.shape}, y_val shape: {y_val_isPhoto.shape}\")\n",
    "\n",
    "loss_function = \"mse\";\n",
    "layers_dims = [x_train_isPhoto.shape[-1], 32, 16, 1] # linear for converge, sigmoid for diverge\n",
    "activation_fn = [\"relu\", \"relu\", \"linear\"]\n",
    "learning_rate = 0.05\n",
    "num_iterations = 50000\n",
    "print_loss = True\n",
    "print_freq = 1000\n",
    "decrease_freq = 1000\n",
    "decrease_proportion = 0.9\n",
    "batch_size = 16 \n",
    "\n",
    "model_isPhoto = Model(layers_dims, activation_fn, loss_function)\n",
    "model_isPhoto, losses, history = train_model(model_isPhoto, x_train_isPhoto, y_train_isPhoto, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MAPE(prediction, ground_truth):\n",
    "    ground_truth = np.where(ground_truth == 0, np.finfo(float).eps, ground_truth) # ensure no 0 in nparray\n",
    "    mape = np.sum(np.abs((ground_truth - prediction) / ground_truth)) / len(prediction) * 100\n",
    "    return mape\n",
    "\n",
    "def predict(x, y_true, model):\n",
    "    # 預測\n",
    "    y_pred = model.forward(x)\n",
    "    \n",
    "    if y_true is not None:\n",
    "        # Mean Squared Error\n",
    "        mse = np.mean(np.square(y_pred - y_true))\n",
    "        print(f\"MSE : {mse}\")\n",
    "\n",
    "        # Mean Absolute Error\n",
    "        mae = np.mean(np.abs(y_pred - y_true))\n",
    "        print(f\"MAE : {mae}\")\n",
    "\n",
    "        mape = calculate_MAPE(y_true, y_pred)\n",
    "        print(f\"MAPE {mape:.2f}%\")\n",
    "\n",
    "        # Accuracy (within tolerance)\n",
    "        # tolerance = 1e-3 \n",
    "        # correct = np.sum(np.isclose(y_pred.flatten(), y_true.flatten(), atol=tolerance))\n",
    "        # accuracy = correct / len(y_true) * 100\n",
    "        # print(f\"Accuracy (within tolerance of {tolerance}): {accuracy:.2f}%\")\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load validation data\n",
    "# data_val_root = \"out.csv\"\n",
    "# with open(data_val_root, newline='') as csvfile:\n",
    "#     datalist_val = pd.read_csv(data_val_root)\n",
    "\n",
    "# filtered_data_val = datalist_val[(datalist_val[\"followers\"] > 10000) & (datalist_val[\"followers\"] < 1000000)]\n",
    "# data_val_isVideo = filtered_data_val[filtered_data_val[\"is Photo\"] == 0]\n",
    "# xy_val_isVideo = data_val_isVideo[[ \"followers\", \"hashtag_count\", \"days_diff\", \"Like Count\"]].to_numpy()\n",
    "\n",
    "# x_val_isVideo = xy_val_isVideo[:, :-1]  \n",
    "# y_val_isVideo = xy_val_isVideo[:, -1]   \n",
    "# x_val_processed_isVideo, y_processed_isVideo = preProcessed(x_isVideo, y_isVideo)\n",
    "\n",
    "# data_val_isPhoto = filtered_data_val[filtered_data_val[\"is Photo\"] == 1]\n",
    "# xy_val_isPhoto = data_val_isPhoto[[ \"followers\", \"hashtag_count\", \"days_diff\", \"Like Count\"]].to_numpy()\n",
    "\n",
    "# x_val_isPhoto = xy_val_isPhoto[:, :-1]  \n",
    "# y_val_isPhoto = xy_val_isPhoto[:, -1]   \n",
    "# x_val_processed_isPhoto, y_processed_isPhoto = preProcessed(x_isPhoto, y_isPhoto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 18966,
     "status": "ok",
     "timestamp": 1730912432655,
     "user": {
      "displayName": "賴詠欣",
      "userId": "11548507785211494753"
     },
     "user_tz": -480
    },
    "id": "0uwle3uqL9Em",
    "outputId": "d753b672-494a-4d1b-a6a0-bd1f99ae0f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo post predict:\n",
      "training data\n",
      "MSE : 9.178563999396443e-07\n",
      "MAE : 0.0005346917313302315\n",
      "MAPE 2.37%\n",
      "validation data\n",
      "MSE : 0.007117270844143761\n",
      "MAE : 0.0053686063916422375\n",
      "MAPE 23.18%\n",
      "video post predict:\n",
      "training data\n",
      "MSE : 0.020283275841280646\n",
      "MAE : 0.0694596549324303\n",
      "MAPE 50.03%\n",
      "validation data\n",
      "MSE : 0.21474430196384442\n",
      "MAE : 0.23798399817368646\n",
      "MAPE 90.04%\n"
     ]
    }
   ],
   "source": [
    "print(\"photo post predict:\")\n",
    "print(\"training data\")\n",
    "pred_train = predict(x_train_isPhoto, y_train_isPhoto, model_isPhoto)\n",
    "print(\"validation data\")\n",
    "pred_val = predict(x_val_isPhoto, y_val_isPhoto, model_isPhoto)\n",
    "print(\"video post predict:\")\n",
    "print(\"training data\")\n",
    "pred_train = predict(x_train_isVideo, y_train_isVideo, model_isVideo)\n",
    "print(\"validation data\")\n",
    "pred_val = predict(x_val_isVideo, y_val_isVideo, model_isVideo)\n",
    "# save_final_result(model, x_train, y_train)\n",
    "# animate_training(history, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction data saved as 'training_output.csv'\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'ID': range(len(pred_train)),\n",
    "    'Label': pred_train.flatten()\n",
    "})\n",
    "\n",
    "df.to_csv('training_output.csv', index=False)\n",
    "print(\"Prediction data saved as 'training_output.csv'\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
